{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FB_Covid.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPUM79x+RSEx+01Q8ulJOmF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanmayg/LS/blob/master/FB_Covid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeNU4Ode7l7k"
      },
      "source": [
        "!pip install pytorch_lightning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkJV1bUsOiuT"
      },
      "source": [
        "import logging\r\n",
        "import os\r\n",
        "from argparse import ArgumentParser\r\n",
        "from pathlib import Path\r\n",
        "from warnings import warn\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pytorch_lightning as pl\r\n",
        "import torch\r\n",
        "import yaml\r\n",
        "#import argparse\r\n",
        "\r\n",
        "#argparser = argparse.ArgumentParser()"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omthm1q7rf0z",
        "outputId": "851c39fc-8e98-4cdf-ffa9-9426b121b53b"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08Yh_wU8Pnrp"
      },
      "source": [
        "from transforms import (\r\n",
        "    Compose,\r\n",
        "    HistogramNormalize,\r\n",
        "    NanToInt,\r\n",
        "    RemapLabel,\r\n",
        "    TensorToRGB,\r\n",
        ")"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm1esry_P0WQ"
      },
      "source": [
        "from xray_datamodule import XrayDataModule"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6Lg0rrkP52x"
      },
      "source": [
        "from torchvision import transforms\r\n",
        "from sip_finetune import SipModule"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZY0iJaISFgY"
      },
      "source": [
        "#!python train_sip.py --pretrained_file mimic-chexpert_lr_0.01_bs_128_fd_128_qs_65536.pt"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_HfjWKGER58"
      },
      "source": [
        "argparser.add_argument(\"--pretrained_file\", help=\"Pretrained File\", default=\"mimic-chexpert_lr_0.01_bs_128_fd_128_qs_65536.pt\")\r\n",
        "argparser.add_argument(\"--im_size\", default=224, type=int)\r\n",
        "argparser.add_argument(\"--uncertain_label\", default=np.nan, type=float)\r\n",
        "argparser.add_argument(\"--nan_label\", default=np.nan, type=float)\r\n",
        "args = argparser.parse_args([\"--pretrained_file\", \"mimic-chexpert_lr_0.01_bs_128_fd_128_qs_65536.pt\"])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IU2p7YX7AjO6",
        "outputId": "ff43fa97-656e-4886-83d4-b84d80ecdc84"
      },
      "source": [
        "args"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Namespace(im_size=224, nan_label=nan, pretrained_file='mimic-chexpert_lr_0.01_bs_128_fd_128_qs_65536.pt', uncertain_label=nan)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gr4ffHStjwJ"
      },
      "source": [
        "def build_args(arg_defaults=None):\r\n",
        "    pl.seed_everything(1234)\r\n",
        "    data_config = Path.cwd() / \"data.yaml\"\r\n",
        "    tmp = arg_defaults\r\n",
        "    arg_defaults = {\r\n",
        "        \"accelerator\": \"ddp\",\r\n",
        "        \"batch_size\": 32,\r\n",
        "        \"max_epochs\": 5,\r\n",
        "        \"gpus\": 1,\r\n",
        "        \"num_workers\": 10,\r\n",
        "        \"callbacks\": [],\r\n",
        "    }\r\n",
        "    if tmp is not None:\r\n",
        "        arg_defaults.update(tmp)\r\n",
        "\r\n",
        "    # ------------\r\n",
        "    # args\r\n",
        "    # ------------\r\n",
        "    parser = ArgumentParser()\r\n",
        "    parser.add_argument(\"--im_size\", default=224, type=int)\r\n",
        "    parser.add_argument(\"--uncertain_label\", default=np.nan, type=float)\r\n",
        "    parser.add_argument(\"--nan_label\", default=np.nan, type=float)\r\n",
        "    parser = pl.Trainer.add_argparse_args(parser)\r\n",
        "    parser = XrayDataModule.add_model_specific_args(parser)\r\n",
        "    parser = SipModule.add_model_specific_args(parser)\r\n",
        "    parser.set_defaults(**arg_defaults)\r\n",
        "    args = parser.parse_args()\r\n",
        "\r\n",
        "    if args.default_root_dir is None:\r\n",
        "        args.default_root_dir = Path.cwd()\r\n",
        "\r\n",
        "    if args.pretrained_file is None:\r\n",
        "        warn(\"Pretrained file not specified, training from scratch.\")\r\n",
        "    else:\r\n",
        "        logging.info(f\"Loading pretrained file from {args.pretrained_file}\")\r\n",
        "\r\n",
        "    if args.dataset_dir is None:\r\n",
        "        with open(data_config, \"r\") as f:\r\n",
        "            paths = yaml.load(f, Loader=yaml.SafeLoader)[\"paths\"]\r\n",
        "\r\n",
        "        if args.dataset_name == \"nih\":\r\n",
        "            args.dataset_dir = paths[\"nih\"]\r\n",
        "        if args.dataset_name == \"mimic\":\r\n",
        "            args.dataset_dir = paths[\"mimic\"]\r\n",
        "        elif args.dataset_name == \"chexpert\":\r\n",
        "            args.dataset_dir = paths[\"chexpert\"]\r\n",
        "        elif args.dataset_name == \"mimic-chexpert\":\r\n",
        "            args.dataset_dir = [paths[\"chexpert\"], paths[\"mimic\"]]\r\n",
        "        else:\r\n",
        "            raise ValueError(\"Unrecognized path config.\")\r\n",
        "\r\n",
        "    if args.dataset_name in (\"chexpert\", \"mimic\", \"mimic-chexpert\"):\r\n",
        "        args.val_pathology_list = [\r\n",
        "            \"Atelectasis\",\r\n",
        "            \"Cardiomegaly\",\r\n",
        "            \"Consolidation\",\r\n",
        "            \"Edema\",\r\n",
        "            \"Pleural Effusion\",\r\n",
        "        ]\r\n",
        "    elif args.dataset_name == \"nih\":\r\n",
        "        args.val_pathology_list = [\r\n",
        "            \"Atelectasis\",\r\n",
        "            \"Cardiomegaly\",\r\n",
        "            \"Consolidation\",\r\n",
        "            \"Edema\",\r\n",
        "            \"Effusion\",\r\n",
        "        ]\r\n",
        "    else:\r\n",
        "        raise ValueError(\"Unrecognized dataset.\")\r\n",
        "\r\n",
        "    # ------------\r\n",
        "    # checkpoints\r\n",
        "    # ------------\r\n",
        "    checkpoint_dir = Path(args.default_root_dir) / \"checkpoints\"\r\n",
        "    if not checkpoint_dir.exists():\r\n",
        "        checkpoint_dir.mkdir(parents=True)\r\n",
        "    elif args.resume_from_checkpoint is None:\r\n",
        "        ckpt_list = sorted(checkpoint_dir.glob(\"*.ckpt\"), key=os.path.getmtime)\r\n",
        "        if ckpt_list:\r\n",
        "            args.resume_from_checkpoint = str(ckpt_list[-1])\r\n",
        "\r\n",
        "    args.callbacks.append(\r\n",
        "        pl.callbacks.ModelCheckpoint(dirpath=checkpoint_dir, verbose=True)\r\n",
        "    )\r\n",
        "\r\n",
        "    return args\r\n",
        "\r\n",
        "def fetch_pos_weights(dataset_name, csv, label_list, uncertain_label, nan_label):\r\n",
        "    if dataset_name == \"nih\":\r\n",
        "        pos = [(csv[\"Finding Labels\"].str.contains(lab)).sum() for lab in label_list]\r\n",
        "        neg = [(~csv[\"Finding Labels\"].str.contains(lab)).sum() for lab in label_list]\r\n",
        "        pos_weights = torch.tensor((neg / np.maximum(pos, 1)).astype(np.float))\r\n",
        "    else:\r\n",
        "        pos = (csv[label_list] == 1).sum()\r\n",
        "        neg = (csv[label_list] == 0).sum()\r\n",
        "\r\n",
        "        if uncertain_label == 1:\r\n",
        "            pos = pos + (csv[label_list] == -1).sum()\r\n",
        "        elif uncertain_label == -1:\r\n",
        "            neg = neg + (csv[label_list] == -1).sum()\r\n",
        "\r\n",
        "        if nan_label == 1:\r\n",
        "            pos = pos + (csv[label_list].isna()).sum()\r\n",
        "        elif nan_label == -1:\r\n",
        "            neg = neg + (csv[label_list].isna()).sum()\r\n",
        "\r\n",
        "        pos_weights = torch.tensor((neg / np.maximum(pos, 1)).values.astype(np.float))\r\n",
        "\r\n",
        "    return pos_weights"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdFLdQbg36VJ"
      },
      "source": [
        "#args = build_args()\r\n",
        "im_size = 224\r\n",
        "dataset_name = \"mimic\"\r\n",
        "uncertain_label = np.nan\r\n",
        "nan_label = np.nan\r\n",
        "pretrained_file = \"mimic-chexpert_lr_0.01_bs_128_fd_128_qs_65536.pt\"\r\n",
        "batch_size = 64\r\n",
        "num_workers = 4"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZCe7nMoyks6"
      },
      "source": [
        "train_transform_list = [\r\n",
        "        transforms.Resize(im_size),\r\n",
        "        transforms.CenterCrop(im_size),\r\n",
        "        transforms.RandomHorizontalFlip(),\r\n",
        "        transforms.RandomVerticalFlip(),\r\n",
        "        transforms.ToTensor(),\r\n",
        "        HistogramNormalize(),\r\n",
        "        TensorToRGB(),\r\n",
        "        RemapLabel(-1, uncertain_label),\r\n",
        "        NanToInt(nan_label),\r\n",
        "    ]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV2wRLYl0Eqc"
      },
      "source": [
        "val_transform_list = [\r\n",
        "        transforms.Resize(im_size),\r\n",
        "        transforms.CenterCrop(im_size),\r\n",
        "        transforms.ToTensor(),\r\n",
        "        HistogramNormalize(),\r\n",
        "        TensorToRGB(),\r\n",
        "        RemapLabel(-1, uncertain_label),\r\n",
        "    ]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5ma-aTC8JP1"
      },
      "source": [
        "with open(\"data.yaml\", \"r\") as f:\r\n",
        "  paths = yaml.load(f, Loader=yaml.SafeLoader)[\"paths\"]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoyoYpAp8S7q"
      },
      "source": [
        "if dataset_name == \"nih\":\r\n",
        "  dataset_dir = paths[\"nih\"]\r\n",
        "if dataset_name == \"mimic\":\r\n",
        "  dataset_dir = paths[\"mimic\"]\r\n",
        "elif dataset_name == \"chexpert\":\r\n",
        "  dataset_dir = paths[\"chexpert\"]\r\n",
        "elif dataset_name == \"mimic-chexpert\":\r\n",
        "  dataset_dir = [paths[\"chexpert\"], paths[\"mimic\"]]\r\n",
        "else:\r\n",
        "  raise ValueError(\"Unrecognized path config.\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh8Hcwvy8l6d"
      },
      "source": [
        "if dataset_name in (\"chexpert\", \"mimic\", \"mimic-chexpert\"):\r\n",
        "  val_pathology_list = [\r\n",
        "            \"Atelectasis\",\r\n",
        "            \"Cardiomegaly\",\r\n",
        "            \"Consolidation\",\r\n",
        "            \"Edema\",\r\n",
        "            \"Pleural Effusion\",\r\n",
        "        ]\r\n",
        "elif dataset_name == \"nih\":\r\n",
        "       val_pathology_list = [\r\n",
        "            \"Atelectasis\",\r\n",
        "            \"Cardiomegaly\",\r\n",
        "            \"Consolidation\",\r\n",
        "            \"Edema\",\r\n",
        "            \"Effusion\",\r\n",
        "        ]\r\n",
        "else:\r\n",
        "    raise ValueError(\"Unrecognized dataset.\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BBH5QYl8hrF",
        "outputId": "54df6a4d-1b9b-435d-c70d-b2a663a3b333"
      },
      "source": [
        "dataset_dir, val_pathology_list"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('mimic_data',\n",
              " ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9JScGOOJbjM"
      },
      "source": [
        "\"\"\"\r\n",
        "Copyright (c) Facebook, Inc. and its affiliates.\r\n",
        "\r\n",
        "This source code is licensed under the MIT license found in the\r\n",
        "LICENSE file in the root directory of this source tree.\r\n",
        "\"\"\"\r\n",
        "import os\r\n",
        "from argparse import ArgumentParser\r\n",
        "from typing import Callable, List, Optional, Union\r\n",
        "\r\n",
        "from base_dataset import BaseDataset\r\n",
        "import numpy as np\r\n",
        "import pytorch_lightning as pl\r\n",
        "import torch\r\n",
        "from mimic_cxr import MimicCxrJpgDataset\r\n",
        "\r\n",
        "\r\n",
        "class TwoImageDataset(torch.utils.data.Dataset):\r\n",
        "    \"\"\"\r\n",
        "    Wrapper for returning two augmentations of the same image.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        dataset: Pre-initialized data set to return multiple samples from.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, dataset: BaseDataset):\r\n",
        "        assert isinstance(dataset, BaseDataset)\r\n",
        "        self.dataset = dataset\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.dataset)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        # randomness handled via the transform objects\r\n",
        "        # this requires the transforms to sample randomness from the process\r\n",
        "        # generator\r\n",
        "        item0 = self.dataset[idx]\r\n",
        "        item1 = self.dataset[idx]\r\n",
        "\r\n",
        "        sample = {\r\n",
        "            \"image0\": item0[\"image\"],\r\n",
        "            \"image1\": item1[\"image\"],\r\n",
        "            \"label\": item0[\"labels\"],\r\n",
        "        }\r\n",
        "\r\n",
        "        return sample\r\n",
        "\r\n",
        "\r\n",
        "def fetch_dataset(\r\n",
        "    dataset_name: str,\r\n",
        "    dataset_dir: Union[List[Union[str, os.PathLike]], Union[str, os.PathLike]],\r\n",
        "    split: str,\r\n",
        "    transform: Optional[Callable],\r\n",
        "    two_image: bool = False,\r\n",
        "    label_list=\"all\",\r\n",
        "):\r\n",
        "    \"\"\"Dataset fetcher for config handling.\"\"\"\r\n",
        "\r\n",
        "    assert split in (\"train\", \"val\", \"test\")\r\n",
        "    dataset: Union[BaseDataset, TwoImageDataset]\r\n",
        "\r\n",
        "    # determine the dataset\r\n",
        "    if dataset_name == \"nih\":\r\n",
        "        assert not isinstance(dataset_dir, list)\r\n",
        "        dataset = NIHChestDataset(\r\n",
        "            directory=dataset_dir,\r\n",
        "            split=split,\r\n",
        "            transform=transform,\r\n",
        "            label_list=label_list,\r\n",
        "            resplit=True,\r\n",
        "        )\r\n",
        "    if dataset_name == \"mimic\":\r\n",
        "        assert not isinstance(dataset_dir, list)\r\n",
        "        print(\"label_list from fetch_dataset: \", label_list)\r\n",
        "        dataset = MimicCxrJpgDataset(\r\n",
        "            directory=dataset_dir,\r\n",
        "            split=split,\r\n",
        "            transform=transform,\r\n",
        "            label_list=label_list,\r\n",
        "        )\r\n",
        "    elif dataset_name == \"chexpert\":\r\n",
        "        assert not isinstance(dataset_dir, list)\r\n",
        "        dataset = CheXpertDataset(\r\n",
        "            directory=dataset_dir,\r\n",
        "            split=split,\r\n",
        "            transform=transform,\r\n",
        "            label_list=label_list,\r\n",
        "        )\r\n",
        "    elif dataset_name == \"mimic-chexpert\":\r\n",
        "        assert isinstance(dataset_dir, list)\r\n",
        "        dataset = CombinedXrayDataset(\r\n",
        "            dataset_list=[\"chexpert_v1\", \"mimic-cxr\"],\r\n",
        "            directory_list=dataset_dir,\r\n",
        "            transform_list=[transform, transform],\r\n",
        "            label_list=[label_list, label_list],\r\n",
        "            split_list=[split, split],\r\n",
        "        )\r\n",
        "    else:\r\n",
        "        raise ValueError(f\"dataset {dataset_name} not recognized\")\r\n",
        "\r\n",
        "    if two_image is True:\r\n",
        "        dataset = TwoImageDataset(dataset)\r\n",
        "\r\n",
        "    return dataset\r\n",
        "\r\n",
        "\r\n",
        "def worker_init_fn(worker_id):\r\n",
        "    \"\"\"Handle random seeding.\"\"\"\r\n",
        "    worker_info = torch.utils.data.get_worker_info()\r\n",
        "    seed = worker_info.seed % (2 ** 32 - 1)  # pylint: disable=no-member\r\n",
        "\r\n",
        "    np.random.seed(seed)\r\n",
        "\r\n",
        "\r\n",
        "class XrayDataModule(pl.LightningDataModule):\r\n",
        "    \"\"\"\r\n",
        "    X-ray data module for training models with PyTorch Lightning.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        dataset_name: Name of the dataset.\r\n",
        "        dataset_dir: Location of the data.\r\n",
        "        label_list: Labels to load for training.\r\n",
        "        batch_size: Training batch size.\r\n",
        "        num_workers: Number of workers for dataloaders.\r\n",
        "        use_two_images: Whether to return two augmentations of same image from\r\n",
        "            dataset (for MoCo pretraining).\r\n",
        "        train_transform: Transform for training loop.\r\n",
        "        val_transform: Transform for validation loop.\r\n",
        "        test_transform: Transform for test loop.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        dataset_name: str,\r\n",
        "        dataset_dir: Union[List[Union[str, os.PathLike]], Union[str, os.PathLike]],\r\n",
        "        label_list: Union[str, List[str]] = \"all\",\r\n",
        "        batch_size: int = 1,\r\n",
        "        num_workers: int = 4,\r\n",
        "        use_two_images: bool = False,\r\n",
        "        train_transform: Optional[Callable] = None,\r\n",
        "        val_transform: Optional[Callable] = None,\r\n",
        "        test_transform: Optional[Callable] = None,\r\n",
        "    ):\r\n",
        "        super().__init__()\r\n",
        "        print(\"label_list from XrayDataModule: \", label_list)\r\n",
        "        self.dataset_name = dataset_name\r\n",
        "        self.dataset_dir = dataset_dir\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.num_workers = num_workers\r\n",
        "\r\n",
        "        self.train_dataset = fetch_dataset(\r\n",
        "            self.dataset_name,\r\n",
        "            self.dataset_dir,\r\n",
        "            \"train\",\r\n",
        "            train_transform,\r\n",
        "            label_list=label_list,\r\n",
        "            two_image=use_two_images,\r\n",
        "        )\r\n",
        "        self.val_dataset = fetch_dataset(\r\n",
        "            self.dataset_name,\r\n",
        "            self.dataset_dir,\r\n",
        "            \"val\",\r\n",
        "            val_transform,\r\n",
        "            label_list=label_list,\r\n",
        "            two_image=use_two_images,\r\n",
        "        )\r\n",
        "        self.test_dataset = fetch_dataset(\r\n",
        "            self.dataset_name,\r\n",
        "            self.dataset_dir,\r\n",
        "            \"test\",\r\n",
        "            test_transform,\r\n",
        "            label_list=label_list,\r\n",
        "            two_image=use_two_images,\r\n",
        "        )\r\n",
        "\r\n",
        "        if isinstance(self.train_dataset, TwoImageDataset):\r\n",
        "            self.label_list = None\r\n",
        "        else:\r\n",
        "            self.label_list = self.train_dataset.label_list\r\n",
        "\r\n",
        "    def __dataloader(self, split: str) -> torch.utils.data.DataLoader:\r\n",
        "        assert split in (\"train\", \"val\", \"test\")\r\n",
        "        shuffle = False\r\n",
        "        if split == \"train\":\r\n",
        "            dataset = self.train_dataset\r\n",
        "            shuffle = True\r\n",
        "        elif split == \"val\":\r\n",
        "            dataset = self.val_dataset\r\n",
        "        else:\r\n",
        "            dataset = self.test_dataset\r\n",
        "\r\n",
        "        loader = torch.utils.data.DataLoader(\r\n",
        "            dataset=dataset,\r\n",
        "            batch_size=self.batch_size,\r\n",
        "            num_workers=self.num_workers,\r\n",
        "            drop_last=True,\r\n",
        "            shuffle=shuffle,\r\n",
        "            worker_init_fn=worker_init_fn,\r\n",
        "        )\r\n",
        "\r\n",
        "        return loader\r\n",
        "\r\n",
        "    def train_dataloader(self):\r\n",
        "        return self.__dataloader(split=\"train\")\r\n",
        "\r\n",
        "    def val_dataloader(self):\r\n",
        "        return self.__dataloader(split=\"val\")\r\n",
        "\r\n",
        "    def test_dataloader(self):\r\n",
        "        return self.__dataloader(split=\"test\")\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def add_model_specific_args(parent_parser):\r\n",
        "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\r\n",
        "\r\n",
        "        parser.add_argument(\"--dataset_name\", default=\"mimic\", type=str)\r\n",
        "        parser.add_argument(\"--dataset_dir\", default=None, type=str)\r\n",
        "        parser.add_argument(\"--batch_size\", default=64, type=int)\r\n",
        "        parser.add_argument(\"--num_workers\", default=4, type=int)\r\n",
        "\r\n",
        "        return parser"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvMBAK7G3V6O",
        "outputId": "352ebc78-2af1-4e2a-df7d-59395ee97b33"
      },
      "source": [
        "data_module = XrayDataModule(\r\n",
        "        dataset_name=dataset_name,\r\n",
        "        dataset_dir=dataset_dir,\r\n",
        "        batch_size=batch_size,\r\n",
        "        num_workers=num_workers,\r\n",
        "        train_transform=Compose(train_transform_list),\r\n",
        "        val_transform=Compose(val_transform_list),\r\n",
        "        test_transform=Compose(val_transform_list),\r\n",
        "    )"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label_list from XrayDataModule:  all\n",
            "label_list from fetch_dataset:  all\n",
            "label_list from fetch_dataset:  all\n",
            "label_list from fetch_dataset:  all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2aDX4L0JUoL",
        "outputId": "62e6743e-96fd-45c6-8a35-6c36f2f677f3"
      },
      "source": [
        "data_module.label_list"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['No Finding',\n",
              " 'Enlarged Cardiomediastinum',\n",
              " 'Cardiomegaly',\n",
              " 'Lung Opacity',\n",
              " 'Lung Lesion',\n",
              " 'Edema',\n",
              " 'Consolidation',\n",
              " 'Pneumonia',\n",
              " 'Atelectasis',\n",
              " 'Pneumothorax',\n",
              " 'Pleural Effusion',\n",
              " 'Pleural Other',\n",
              " 'Fracture',\n",
              " 'Support Devices']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3-O-ZKFC-Iz"
      },
      "source": [
        "assert not isinstance(dataset_dir, list)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWOg5MZQDljy"
      },
      "source": [
        "from typing import Callable, List, Optional, Union"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by3PgGdWDqF9"
      },
      "source": [
        "transform = Optional[Callable]"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofPeLtK1DDya"
      },
      "source": [
        "from mimic_cxr import MimicCxrJpgDataset\r\n",
        "dataset = MimicCxrJpgDataset(\r\n",
        "            directory=dataset_dir,\r\n",
        "            split=\"train\",\r\n",
        "            transform=transform,\r\n",
        "            label_list=\"all\",\r\n",
        "        )"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhPHkafsGvcn",
        "outputId": "e8f764a6-d607-497a-e9b9-c637380f0b96"
      },
      "source": [
        "dataset.label_list"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['No Finding',\n",
              " 'Enlarged Cardiomediastinum',\n",
              " 'Cardiomegaly',\n",
              " 'Lung Opacity',\n",
              " 'Lung Lesion',\n",
              " 'Edema',\n",
              " 'Consolidation',\n",
              " 'Pneumonia',\n",
              " 'Atelectasis',\n",
              " 'Pneumothorax',\n",
              " 'Pleural Effusion',\n",
              " 'Pleural Other',\n",
              " 'Fracture',\n",
              " 'Support Devices']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBBlr9JzFOzR",
        "outputId": "acd3fef4-f716-42bb-a22d-ee300af7c657"
      },
      "source": [
        "Union[str, List[str]]"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "typing.Union[str, typing.List[str]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_9XCSPj9PIb"
      },
      "source": [
        "# ------------\r\n",
        "# model\r\n",
        "# ------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UTa7Hki9k7R",
        "outputId": "733c5af5-a079-4806-9294-e11e2b7cb161"
      },
      "source": [
        "fetch_pos_weights(\r\n",
        "    dataset_name=dataset_name,\r\n",
        "    csv=data_module.train_dataset.csv,\r\n",
        "    label_list=data_module.label_list,\r\n",
        "    uncertain_label=uncertain_label,\r\n",
        "    nan_label=nan_label,\r\n",
        ")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0000, 0.8726, 0.3917, 0.0671, 0.1601, 1.1363, 0.9775, 1.6701, 0.0348,\n",
              "        4.0171, 0.5680, 0.0750, 0.2347, 0.0618], dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIjCN_6C3Xng"
      },
      "source": [
        "pos_weights = fetch_pos_weights(\r\n",
        "    dataset_name=dataset_name,\r\n",
        "    csv=data_module.train_dataset.csv,\r\n",
        "    label_list=data_module.label_list,\r\n",
        "    uncertain_label=uncertain_label,\r\n",
        "    nan_label=nan_label,\r\n",
        ")"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHfunD1NKFZ-"
      },
      "source": [
        "arch = \"densenet121\"\r\n",
        "max_epochs = 5"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiUZbxVr9XUi"
      },
      "source": [
        " model = SipModule(\r\n",
        "        arch=arch,\r\n",
        "        num_classes=len(data_module.label_list),\r\n",
        "        pretrained_file=pretrained_file,\r\n",
        "        label_list=data_module.label_list,\r\n",
        "        val_pathology_list=val_pathology_list,\r\n",
        "        learning_rate=learning_rate,\r\n",
        "        pos_weights=pos_weights,\r\n",
        "        epochs=max_epochs,\r\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}